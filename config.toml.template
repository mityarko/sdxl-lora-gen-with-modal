pretrained_model_name_or_path = '/model/＜ベースモデルファイル名＞'
dataset_config = '/input/dataset.toml'
output_dir = '/output'

# hyperparameters
lr_scheduler = "constant_with_warmup"
# lr_scheduler_num_cycles = 5
lr_warmup_steps = 100
max_token_length = 225
max_train_epochs = 10
# optimizer_args = ["weight_decay=0.01", "decouple=True", "use_bias_correction=True"]
optimizer_type = "AdamW"
max_grad_norm = 1.0
# optimizer_args = ["scale_parameter=False", "relative_step=False", "warmup_init=False"]
# optimizer_type = "AdaFactor"
seed = 3407

cache_latents = true
cache_latents_to_disk = true
gradient_checkpointing = true
# min_snr_gamma = 5
lowram = false
mixed_precision = "bf16"
xformers = true

save_every_n_epochs = 1
# save_every_n_steps = 1000
save_last_n_epochs = 5
# save_last_n_steps = 5000

# lora
learning_rate = 1e-5
# unet_lr = 5e-6

network_train_unet_only = true

network_alpha = 1
network_args = ["conv_dim=8", "conv_alpha=1"]
network_dim = 8
network_module = "networks.lora"
# network_weights = ""
persistent_data_loader_workers = true
prior_loss_weight = 1.0

# XL
# cache_text_encoder_outputs = true
